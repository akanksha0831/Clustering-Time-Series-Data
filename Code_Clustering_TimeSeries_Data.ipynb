{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Time Series Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.metrics.cluster import *\n",
    "import random\n",
    "import operator\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange,tqdm\n",
    "import random\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [(\"Colposcopy.csv\",6),(\"ECG200.csv\",2),(\"Lightning2.csv\",2),(\"SharePriceIncrease.csv\",2),(\"Wafer.csv\",2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implementation of time series clustering methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K means Euclidean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeansClusteringE:\n",
    "    \n",
    "    def __init__(self, X, num_clusters, metric=\"Euclidean\", max_iter=100):\n",
    "        self.K = num_clusters\n",
    "        self.max_iterations = max_iter\n",
    "        self.num_examples = X.shape[0]\n",
    "        self.num_features = X.shape[1]\n",
    "        self.metric=metric\n",
    "\n",
    "    \"\"\"random centroids are being initialized\n",
    "    input: dataset\n",
    "    returns: initial centroids\"\"\"\n",
    "    def initialize_random_centroids(self, X):\n",
    "        centroids = np.zeros((self.K, self.num_features))\n",
    "        for k in range(self.K):\n",
    "            centroid = X[np.random.choice(range(self.num_examples))]  #generating random index\n",
    "            centroids[k] = centroid                                   #assigning random examples as the centroids\n",
    "        return centroids\n",
    "    \n",
    "    \"\"\"clusters are being created based on euclidean distance\n",
    "    input: dataset, centroids\n",
    "    returns: clusters\"\"\"\n",
    "    def create_clusters(self, X, centroids):\n",
    "        clusters = [[] for _ in range(self.K)]                        #points associated with specific cluster        \n",
    "        for point_idx, point in enumerate(X):                     #loop through each point and check the closest centroid\n",
    "            closest_centroid = np.argmin(\n",
    "                np.sqrt(np.sum((point - centroids) ** 2, axis=1))     #Euclidean distance using numpy broadcasting\n",
    "            )\n",
    "            clusters[closest_centroid].append(point_idx)\n",
    "        return clusters\n",
    "\n",
    "    \"\"\"calculating new centroids from the available clusters\n",
    "    input: clusters, dataset\n",
    "    returns: new centroids calculated\"\"\"\n",
    "    def calculate_new_centroids(self, clusters, X):\n",
    "        centroids = np.zeros((self.K, self.num_features))\n",
    "        for idx, cluster in enumerate(clusters):\n",
    "            new_centroid = np.mean(X[cluster], axis=0)                #calculating new centroid for each cluster by averaging\n",
    "            centroids[idx] = new_centroid\n",
    "        return centroids\n",
    "\n",
    "    \"\"\"final prediction of class labels for each point\n",
    "    input: clusters, dataset\n",
    "    returns: predicted labels for each point\"\"\"\n",
    "    def predict_cluster(self, clusters, X):\n",
    "        y_pred = np.zeros(self.num_examples)\n",
    "        for cluster_idx, cluster in enumerate(clusters):\n",
    "            for sample_idx in cluster:\n",
    "                y_pred[sample_idx] = cluster_idx+1                     #assigning class label to each point \n",
    "        return y_pred\n",
    "\n",
    "    \"\"\"basically this is the k-means function named as fit\n",
    "    input: dataset\n",
    "    returns: predicted labels for each point obtained from cluster_prediction function\"\"\"\n",
    "    def fit(self, X):\n",
    "        centroids = self.initialize_random_centroids(X)\n",
    "        #print(centroids)\n",
    "        for it in range(self.max_iterations):\n",
    "            clusters = self.create_clusters(X, centroids)\n",
    "            previous_centroids = centroids\n",
    "            centroids = self.calculate_new_centroids(clusters, X)\n",
    "            diff = centroids - previous_centroids\n",
    "            if not diff.any():\n",
    "                break               \n",
    "        # Get label predictions\n",
    "        y_pred = self.predict_cluster(clusters, X)\n",
    "        return y_pred\n",
    "    \n",
    "#O(mnk)-->O(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-means Clustering with Euclidean Distance:\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "667772e3f4b24f51a22cabca355acd7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "              Dataset       ARI       AMI       FMS  Homogenity  Completeness\n",
      "0          Colposcopy  0.090810  0.100538  0.288167    0.134597  1.419133e-01\n",
      "1              ECG200  0.119568  0.054011  0.666569    0.051059  6.798699e-02\n",
      "2          Lightning2  0.011740  0.011933  0.513127    0.018270  1.774633e-02\n",
      "3  SharePriceIncrease -0.001123  0.000173  0.753539    0.000627  4.777712e-02\n",
      "4               Wafer  0.000009 -0.001076  0.687585    0.000001  6.664530e-07\n"
     ]
    }
   ],
   "source": [
    "print(\"K-means Clustering with Euclidean Distance:\")\n",
    "print(\"--------------------------------------------\")\n",
    "\n",
    "res={\"Dataset\":[],\"ARI\":[],\"AMI\":[],\"FMS\":[],\"Homogenity\":[],\"Completeness\":[]}\n",
    "kEARI,kEAMI,kEFMS,kEH,kEC=[],[],[],[],[]\n",
    "for i in trange(len(datasets)):\n",
    "    data=datasets[i]\n",
    "    dataset=pd.read_csv(data[0])\n",
    "    res[\"Dataset\"].append(data[0][:-4])\n",
    "    cols=dataset.shape[1]\n",
    "    x=dataset.iloc[:,:cols-1].values\n",
    "    y=dataset.iloc[:,-1].values\n",
    "    \n",
    "    np.random.seed(10)\n",
    "    num_clusters=data[1]\n",
    "    KMeans=KMeansClusteringE(x,num_clusters)\n",
    "    y_pred=KMeans.fit(x)\n",
    "    \n",
    "    ARI=adjusted_rand_score(y,y_pred)\n",
    "    res[\"ARI\"].append(ARI)\n",
    "    AMI=adjusted_mutual_info_score(y,y_pred,average_method='arithmetic')\n",
    "    res[\"AMI\"].append(AMI)\n",
    "    FMI=fowlkes_mallows_score(y,y_pred,sparse=False)\n",
    "    res[\"FMS\"].append(FMI)\n",
    "    HOMOGENITY=homogeneity_score(y,y_pred)\n",
    "    res[\"Homogenity\"].append(HOMOGENITY)\n",
    "    COMPLETENESS=completeness_score(y,y_pred)\n",
    "    res[\"Completeness\"].append(COMPLETENESS)\n",
    "    \n",
    "    kEARI.append(ARI)\n",
    "    kEAMI.append(AMI)\n",
    "    kEFMS.append(FMI)\n",
    "    kEH.append(HOMOGENITY)\n",
    "    kEC.append(COMPLETENESS)\n",
    "    \n",
    "df=pd.DataFrame(res)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average and Standard deviation of different metrics\n",
      "ARI\n",
      "------\n",
      "avg: 0.04\n",
      "std: 0.06\n",
      "AMI\n",
      "------\n",
      "avg: 0.03\n",
      "std: 0.04\n",
      "FMS\n",
      "------\n",
      "avg: 0.58\n",
      "std: 0.19\n",
      "HOMOGENITY\n",
      "------\n",
      "avg: 0.04\n",
      "std: 0.06\n",
      "COMPLETENESS\n",
      "------\n",
      "avg: 0.06\n",
      "std: 0.06\n"
     ]
    }
   ],
   "source": [
    "print(\"Average and Standard deviation of different metrics\")\n",
    "print(\"ARI\")\n",
    "print(\"------\")\n",
    "print(\"avg:\",round(statistics.mean(kEARI),2))\n",
    "print(\"std:\",round(statistics.stdev(kEARI),2))\n",
    "print(\"AMI\")\n",
    "print(\"------\")\n",
    "print(\"avg:\",round(statistics.mean(kEAMI),2))\n",
    "print(\"std:\",round(statistics.stdev(kEAMI),2))\n",
    "print(\"FMS\")\n",
    "print(\"------\")\n",
    "print(\"avg:\",round(statistics.mean(kEFMS),2))\n",
    "print(\"std:\",round(statistics.stdev(kEFMS),2))\n",
    "print(\"HOMOGENITY\")\n",
    "print(\"------\")\n",
    "print(\"avg:\",round(statistics.mean(kEH),2))\n",
    "print(\"std:\",round(statistics.stdev(kEH),2))\n",
    "print(\"COMPLETENESS\")\n",
    "print(\"------\")\n",
    "print(\"avg:\",round(statistics.mean(kEC),2))\n",
    "print(\"std:\",round(statistics.stdev(kEC),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K means DTW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Dynamic Time warping distance measure for k means\n",
    "   input: two time series and window size\n",
    "   returns: distance between two time series for given window size'''\n",
    "\n",
    "def DTWDistance(s1, s2, w):\n",
    "    DTW={}    # stores the similarity measures \n",
    "    w = max(w, abs(len(s1)-len(s2)))    \n",
    "    for i in range(-1,len(s1)):\n",
    "        for j in range(-1,len(s2)):\n",
    "            DTW[(i, j)] = float('inf')\n",
    "    DTW[(-1, -1)] = 0  \n",
    "    for i in range(len(s1)):\n",
    "        for j in range(max(0, i-w), min(len(s2), i+w)):\n",
    "            dist= (s1[i]-s2[j])**2\n",
    "            # DTW[i, j] is the distance between s1[1:i] and s2[1:j] with the best alignment.\n",
    "            DTW[(i, j)] = dist + min(DTW[(i-1, j)],      # increment\n",
    "                                     DTW[(i, j-1)],      # decrement\n",
    "                                     DTW[(i-1, j-1)])    # match\n",
    "    return np.sqrt(DTW[len(s1)-1, len(s2)-1])\n",
    "\n",
    "class KMeansClusteringD:\n",
    "    \n",
    "    def __init__(self, X, num_clusters, max_iter=100):\n",
    "        self.K = num_clusters\n",
    "        self.max_iterations = max_iter\n",
    "        self.num_examples = X.shape[0]\n",
    "        self.num_features = X.shape[1]\n",
    "\n",
    "    \"\"\"random centroids are being initialized\n",
    "    input: dataset\n",
    "    returns: initial centroids\"\"\"\n",
    "    def initialize_random_centroids(self, X):\n",
    "        centroids = np.zeros((self.K, self.num_features))\n",
    "        for k in range(self.K):\n",
    "            centroid = X[np.random.choice(range(self.num_examples))]  #generating random index\n",
    "            centroids[k] = centroid                                   #assigning random examples as the centroids\n",
    "        return centroids\n",
    "    \n",
    "    \"\"\"clusters are being created based on euclidean distance\n",
    "    input: dataset, centroids\n",
    "    returns: clusters\"\"\"\n",
    "    def create_clusters(self, X, centroids):\n",
    "        clusters = [[] for _ in range(self.K)]                        #points associated with specific cluster\n",
    "        for idx,point in enumerate(X):                #loop through each point and check the closest centroid\n",
    "            min_dis=float('inf')\n",
    "            w=len(point)*0.05\n",
    "            for idx1,centroid in enumerate(centroids):\n",
    "                d=DTWDistance(point,centroid,int(w))\n",
    "                if d<min_dis:\n",
    "                    min_dis=d\n",
    "                    closest_centroid=idx1\n",
    "            clusters[closest_centroid].append(idx)\n",
    "        return clusters\n",
    "\n",
    "    \"\"\"calculating new centroids from the available clusters\n",
    "    input: clusters, dataset\n",
    "    returns: new centroids calculated\"\"\"\n",
    "    def calculate_new_centroids(self, clusters, X):\n",
    "        centroids = np.zeros((self.K, self.num_features))\n",
    "        for idx, cluster in enumerate(clusters):\n",
    "            new_centroid = np.mean(X[cluster], axis=0)                #calculating new centroid for each cluster by averaging\n",
    "            centroids[idx] = new_centroid\n",
    "        return centroids\n",
    "\n",
    "    \"\"\"final prediction of class labels for each point\n",
    "    input: clusters, dataset\n",
    "    returns: predicted labels for each point\"\"\"\n",
    "    def predict_cluster(self, clusters, X):\n",
    "        y_pred = np.zeros(self.num_examples)\n",
    "        for cluster_idx, cluster in enumerate(clusters):\n",
    "            for sample_idx in cluster:\n",
    "                y_pred[sample_idx] = cluster_idx+1                     #assigning class label to each point \n",
    "        return y_pred\n",
    "\n",
    "    \"\"\"basically this is the k-means function named as fit\n",
    "    input: dataset\n",
    "    returns: predicted labels for each point obtained from cluster_prediction function\"\"\"\n",
    "    def fit(self, X):\n",
    "        centroids = self.initialize_random_centroids(X)\n",
    "        #print(centroids)\n",
    "        for it in range(self.max_iterations):\n",
    "            clusters = self.create_clusters(X, centroids)\n",
    "            previous_centroids = centroids\n",
    "            centroids = self.calculate_new_centroids(clusters, X)\n",
    "            diff = centroids - previous_centroids\n",
    "            if not diff.any():\n",
    "                break               \n",
    "        # Get label predictions\n",
    "        y_pred = self.predict_cluster(clusters, X)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-means Clustering with DTW Distance:\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c510a9c322494c3cbe026fc43a130cec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "              Dataset       ARI       AMI       FMS  Homogenity  Completeness\n",
      "0          Colposcopy  0.086487  0.110571  0.277633    0.144960      0.149555\n",
      "1              ECG200  0.129249  0.060669  0.667804    0.057209      0.075027\n",
      "2          Lightning2  0.060838  0.030511  0.640132    0.031279      0.048329\n",
      "3  SharePriceIncrease  0.012738  0.003187  0.548499    0.007541      0.006681\n",
      "4               Wafer -0.004946 -0.000914  0.686887    0.000241      0.000125\n"
     ]
    }
   ],
   "source": [
    "print(\"K-means Clustering with DTW Distance:\")\n",
    "print(\"--------------------------------------------\")\n",
    "res={\"Dataset\":[],\"ARI\":[],\"AMI\":[],\"FMS\":[],\"Homogenity\":[],\"Completeness\":[]}\n",
    "kDARI,kDAMI,kDFMS,kDH,kDC=[],[],[],[],[]\n",
    "for i in trange(len(datasets)):\n",
    "    data=datasets[i]\n",
    "    dataset=pd.read_csv(data[0])\n",
    "    res[\"Dataset\"].append(data[0][:-4])\n",
    "    \n",
    "    cols=dataset.shape[1]\n",
    "    x=dataset.iloc[:,:cols-1].values\n",
    "    y=dataset.iloc[:,-1].values\n",
    "    \n",
    "    np.random.seed(10)\n",
    "    num_clusters=data[1]\n",
    "    KMeansD=KMeansClusteringD(x,num_clusters)\n",
    "    y_pred=KMeansD.fit(x)\n",
    "    \n",
    "    ARI=adjusted_rand_score(y,y_pred)\n",
    "    res[\"ARI\"].append(ARI)\n",
    "    AMI=adjusted_mutual_info_score(y,y_pred,average_method='arithmetic')\n",
    "    res[\"AMI\"].append(AMI)\n",
    "    FMI=fowlkes_mallows_score(y,y_pred,sparse=False)\n",
    "    res[\"FMS\"].append(FMI)\n",
    "    HOMOGENITY=homogeneity_score(y,y_pred)\n",
    "    res[\"Homogenity\"].append(HOMOGENITY)\n",
    "    COMPLETENESS=completeness_score(y,y_pred)\n",
    "    res[\"Completeness\"].append(COMPLETENESS)\n",
    "    \n",
    "    kDARI.append(ARI)\n",
    "    kDAMI.append(AMI)\n",
    "    kDFMS.append(FMI)\n",
    "    kDH.append(HOMOGENITY)\n",
    "    kDC.append(COMPLETENESS)\n",
    "    \n",
    "df=pd.DataFrame(res)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average and Standard deviation of different metrics\n",
      "ARI\n",
      "------\n",
      "avg: 0.06\n",
      "std: 0.05\n",
      "AMI\n",
      "------\n",
      "avg: 0.04\n",
      "std: 0.05\n",
      "FMS\n",
      "------\n",
      "avg: 0.56\n",
      "std: 0.17\n",
      "HOMOGENITY\n",
      "------\n",
      "avg: 0.05\n",
      "std: 0.06\n",
      "COMPLETENESS\n",
      "------\n",
      "avg: 0.06\n",
      "std: 0.06\n"
     ]
    }
   ],
   "source": [
    "print(\"Average and Standard deviation of different metrics\")\n",
    "print(\"ARI\")\n",
    "print(\"------\")\n",
    "print(\"avg:\",round(statistics.mean(kDARI),2))\n",
    "print(\"std:\",round(statistics.stdev(kDARI),2))\n",
    "print(\"AMI\")\n",
    "print(\"------\")\n",
    "print(\"avg:\",round(statistics.mean(kDAMI),2))\n",
    "print(\"std:\",round(statistics.stdev(kDAMI),2))\n",
    "print(\"FMS\")\n",
    "print(\"------\")\n",
    "print(\"avg:\",round(statistics.mean(kDFMS),2))\n",
    "print(\"std:\",round(statistics.stdev(kDFMS),2))\n",
    "print(\"HOMOGENITY\")\n",
    "print(\"------\")\n",
    "print(\"avg:\",round(statistics.mean(kDH),2))\n",
    "print(\"std:\",round(statistics.stdev(kDH),2))\n",
    "print(\"COMPLETENESS\")\n",
    "print(\"------\")\n",
    "print(\"avg:\",round(statistics.mean(kDC),2))\n",
    "print(\"std:\",round(statistics.stdev(kDC),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K means Shape based distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from numpy.random import randint\n",
    "from numpy.linalg import norm, eigh\n",
    "from numpy.fft import fft, ifft\n",
    "\n",
    "\n",
    "def zscore(a, axis=0, ddof=0):\n",
    "    '''\n",
    "       input: a is the sequence to be z normalized\n",
    "       output: returns the z normalized form of a\n",
    "    '''\n",
    "    a = np.asanyarray(a)\n",
    "    mns = a.mean(axis=axis)\n",
    "    sstd = a.std(axis=axis, ddof=ddof)\n",
    "    if axis and mns.ndim < a.ndim:\n",
    "        res = ((a - np.expand_dims(mns, axis=axis)) / np.expand_dims(sstd, axis=axis))\n",
    "    else:\n",
    "        res = (a - mns) / sstd         # (actual - mean)/standard deviation\n",
    "    return np.nan_to_num(res)\n",
    "\n",
    "\n",
    "def roll_zeropad(a, shift, axis=None):\n",
    "    '''\n",
    "       input: a is the sequence that is to be aligned according to shift\n",
    "       output: returns aligned sequence of a according to shift\n",
    "    '''\n",
    "    a = np.asanyarray(a)\n",
    "    if shift == 0:\n",
    "        return a\n",
    "    \n",
    "    if axis is None:\n",
    "        n = a.size\n",
    "        reshape = True\n",
    "    else:\n",
    "        n = a.shape[axis]\n",
    "        reshape = False\n",
    "        \n",
    "    if np.abs(shift) > n:\n",
    "        res = np.zeros_like(a)\n",
    "    elif shift < 0:    # if shift <0 then y = [y(1 − shift : end), zeros(1,−shift)] \n",
    "        shift += n\n",
    "        zeros = np.zeros_like(a.take(np.arange(n-shift), axis))\n",
    "        res = np.concatenate((a.take(np.arange(n-shift, n), axis), zeros), axis)\n",
    "    else:              # if shift ≥ 0 then y = [zeros(1, shift), y(1 : end − shift)]\n",
    "        zeros = np.zeros_like(a.take(np.arange(n-shift, n), axis))\n",
    "        res = np.concatenate((zeros, a.take(np.arange(n-shift), axis)), axis)\n",
    "        \n",
    "    if reshape:\n",
    "        return res.reshape(a.shape)\n",
    "    else:\n",
    "        return res\n",
    "\n",
    "\n",
    "def _ncc_c(x, y):\n",
    "    '''\n",
    "       input: two time series x and y both of 1 dimensional\n",
    "       output: returns coefficient normalization of x and y\n",
    "    '''\n",
    "    den = np.array(norm(x) * norm(y))\n",
    "    den[den == 0] = np.Inf\n",
    "\n",
    "    x_len = len(x)\n",
    "    fft_size = 1 << (2*x_len-1).bit_length()                 # length = 2nextpower2(2∗length(x)−1)\n",
    "\n",
    "    cc = ifft(fft(x, fft_size) * np.conj(fft(y, fft_size)))  # CC = IFFT{FFT(x,length) ∗ FFT(y,length)}\n",
    "    cc = np.concatenate((cc[-(x_len-1):], cc[:x_len]))       \n",
    "    return np.real(cc) / den                                 #NCCc = CC/(||x|| ||y||)\n",
    "\n",
    "\n",
    "def _ncc_c_3dim(x, y):\n",
    "    '''\n",
    "       input: two arrays x and y both of 2 dimensional\n",
    "       output: returns coefficient normalization of x and y\n",
    "    '''\n",
    "    den = norm(x, axis=1)[:, None] * norm(y, axis=1)\n",
    "    den[den == 0] = np.Inf\n",
    "    x_len = x.shape[-1]\n",
    "    fft_size = 1 << (2*x_len-1).bit_length()\n",
    "    cc = ifft(fft(x, fft_size) * np.conj(fft(y, fft_size))[:, None])\n",
    "    cc = np.concatenate((cc[:,:,-(x_len-1):], cc[:,:,:x_len]), axis=2)\n",
    "    return np.real(cc) / den.T[:, :, None]\n",
    "\n",
    "\n",
    "def _sbd(x, y):\n",
    "    '''\n",
    "       Input: Two z-normalized sequences x and y\n",
    "       Output: Dissimilarity dist of x and y \n",
    "               Aligned sequence y1 of y towards x\n",
    "    '''\n",
    "    ncc = _ncc_c(x, y)     # coefficient normalization of x and y and it gives values between -1 and 1\n",
    "    idx = ncc.argmax()     # position where ncc is maximized\n",
    "    dist = 1 - ncc[idx]    # it is the sbd distance which may be between 0 and 2 indicating similarity\n",
    "    yshift = roll_zeropad(y, (idx + 1) - max(len(x), len(y)))    # aligning the sequence y towards x\n",
    "\n",
    "    return dist, yshift\n",
    "\n",
    "\n",
    "def _extract_shape(idx, x, j, cur_center):\n",
    "    '''\n",
    "       Input: idx is an n-by-1 vector containing the assignment of n time series to k clusters.\n",
    "              x is an n-by-m matrix with z-normalized time series.\n",
    "              j is the label of cur_center.\n",
    "              cur_center is a 1-by-m vector with the reference sequence against which time series of x are aligned.\n",
    "       Output: centroid is a z-normalized 1-by-m vector with the centroid.\n",
    "    ''' \n",
    "    _a = []\n",
    "    for i in range(len(idx)):            # for each label in idx array\n",
    "        if idx[i] == j:                  # if the label matches cur_center label\n",
    "            if cur_center.sum() == 0:    # if the sum of values in cur_center is 0\n",
    "                opt_x = x[i]             # use x[i] itself\n",
    "            else:\n",
    "                _, opt_x = _sbd(cur_center, x[i])   # else apply shape based distance for cur_center and x[i]\n",
    "            _a.append(opt_x)\n",
    "    a = np.array(_a)                     # a contains all the points aligned towards the cur_center\n",
    "\n",
    "    if len(a) == 0:\n",
    "        return np.zeros((1, x.shape[1]))\n",
    "    columns = a.shape[1]\n",
    "    y = zscore(a, axis=1, ddof=1)        # z-normalization of array a\n",
    "    s = np.dot(y.transpose(), y)         # dot product of y transpose and y\n",
    "\n",
    "    p = np.empty((columns, columns))\n",
    "    p.fill(1.0/columns)\n",
    "    p = np.eye(columns) - p              # p = I - (1/m)O ---> I is the identity matrix, m is no. of columns, O is matrix with all ones\n",
    "\n",
    "    m = np.dot(np.dot(p, s), p)          # dot product of (p,s) and p\n",
    "    _, vec = eigh(m)                     # eigen value and eigen vector of m is returned\n",
    "    centroid = vec[:, -1]                # normalized eigen vector corresponding to the eigen value\n",
    "    finddistance1 = math.sqrt(((a[0] - centroid) ** 2).sum())\n",
    "    finddistance2 = math.sqrt(((a[0] + centroid) ** 2).sum())\n",
    "\n",
    "    if finddistance1 >= finddistance2:\n",
    "        centroid *= -1\n",
    "\n",
    "    return zscore(centroid, ddof=1)\n",
    "\n",
    "def kshape(x, k):\n",
    "    '''\n",
    "    Input: x is an n-by-m matrix containing n time series of length m that are initially z-normalized.\n",
    "           k is the number of clusters to produce.\n",
    "    Output: idx is an n-by-1 vector containing the assignment of n time series to k clusters (initialized randomly).\n",
    "            centroids is a k-by-m matrix containing k centroids of length m (initialized as vectors with all zeros).\n",
    "    '''\n",
    "    n = x.shape[0] \n",
    "    idx = randint(0, k, size=n)\n",
    "    centroids = np.zeros((k, x.shape[1]))\n",
    "    distances = np.empty((n, k))\n",
    "    for _ in range(100):              # loop until condition is satisfied\n",
    "        #refinement step\n",
    "        old_idx = idx\n",
    "        for j in range(k):            # loop for each centroid\n",
    "            centroids[j] = _extract_shape(idx, x, j, centroids[j])         # calculate each centroid using extract shape func. \n",
    "        \n",
    "        #assignment step\n",
    "        distances = (1 - _ncc_c_3dim(x, centroids).max(axis=2)).T          # dissimilarity distances of x and centroids\n",
    "        idx = distances.argmin(1)     # label of the centroid with minimum distance for each point is stored in idx\n",
    "        \n",
    "        if np.array_equal(old_idx, idx):\n",
    "            break\n",
    "\n",
    "    for i in range(len(idx)):\n",
    "        idx[i]+=1\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-means Clustering with Shape Based Distance:\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffbd26895afc427abdfbc3a58b188bd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "              Dataset       ARI       AMI       FMS  Homogenity  Completeness\n",
      "0          Colposcopy  0.024521  0.037213  0.201862    0.076475      0.072848\n",
      "1              ECG200  0.205360  0.117391  0.651226    0.119180      0.122754\n",
      "2          Lightning2  0.056227  0.075928  0.547992    0.081272      0.082190\n",
      "3  SharePriceIncrease  0.000804 -0.000350  0.545688    0.000054      0.000050\n",
      "4               Wafer  0.014819  0.000616  0.689780    0.002490      0.001274\n"
     ]
    }
   ],
   "source": [
    "print(\"K-means Clustering with Shape Based Distance:\")\n",
    "print(\"--------------------------------------------\")\n",
    "res={\"Dataset\":[],\"ARI\":[],\"AMI\":[],\"FMS\":[],\"Homogenity\":[],\"Completeness\":[]}\n",
    "kSARI,kSAMI,kSFMS,kSH,kSC=[],[],[],[],[]\n",
    "for i in trange(len(datasets)):\n",
    "    data=datasets[i]\n",
    "    dataset=pd.read_csv(data[0])\n",
    "    res[\"Dataset\"].append(data[0][:-4])\n",
    "    cols=dataset.shape[1]\n",
    "    x=dataset.iloc[:,:cols-1].values\n",
    "    y=dataset.iloc[:,-1].values\n",
    "    \n",
    "    np.random.seed(10)\n",
    "    num_clusters=data[1]\n",
    "    y_pred=kshape(zscore(x,axis=1),num_clusters)\n",
    "    \n",
    "    ARI=adjusted_rand_score(y,y_pred)\n",
    "    res[\"ARI\"].append(ARI)\n",
    "    AMI=adjusted_mutual_info_score(y,y_pred,average_method='arithmetic')\n",
    "    res[\"AMI\"].append(AMI)\n",
    "    FMI=fowlkes_mallows_score(y,y_pred,sparse=False)\n",
    "    res[\"FMS\"].append(FMI)\n",
    "    HOMOGENITY=homogeneity_score(y,y_pred)\n",
    "    res[\"Homogenity\"].append(HOMOGENITY)\n",
    "    COMPLETENESS=completeness_score(y,y_pred)\n",
    "    res[\"Completeness\"].append(COMPLETENESS)\n",
    "    \n",
    "    kSARI.append(ARI)\n",
    "    kSAMI.append(AMI)\n",
    "    kSFMS.append(FMI)\n",
    "    kSH.append(HOMOGENITY)\n",
    "    kSC.append(COMPLETENESS)\n",
    "    \n",
    "df=pd.DataFrame(res)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average and Standard deviation of different metrics\n",
      "ARI\n",
      "------\n",
      "avg: 0.06\n",
      "std: 0.08\n",
      "AMI\n",
      "------\n",
      "avg: 0.05\n",
      "std: 0.05\n",
      "FMS\n",
      "------\n",
      "avg: 0.53\n",
      "std: 0.19\n",
      "HOMOGENITY\n",
      "------\n",
      "avg: 0.06\n",
      "std: 0.05\n",
      "COMPLETENESS\n",
      "------\n",
      "avg: 0.06\n",
      "std: 0.05\n"
     ]
    }
   ],
   "source": [
    "print(\"Average and Standard deviation of different metrics\")\n",
    "print(\"ARI\")\n",
    "print(\"------\")\n",
    "print(\"avg:\",round(statistics.mean(kSARI),2))\n",
    "print(\"std:\",round(statistics.stdev(kSARI),2))\n",
    "print(\"AMI\")\n",
    "print(\"------\")\n",
    "print(\"avg:\",round(statistics.mean(kSAMI),2))\n",
    "print(\"std:\",round(statistics.stdev(kSAMI),2))\n",
    "print(\"FMS\")\n",
    "print(\"------\")\n",
    "print(\"avg:\",round(statistics.mean(kSFMS),2))\n",
    "print(\"std:\",round(statistics.stdev(kSFMS),2))\n",
    "print(\"HOMOGENITY\")\n",
    "print(\"------\")\n",
    "print(\"avg:\",round(statistics.mean(kSH),2))\n",
    "print(\"std:\",round(statistics.stdev(kSH),2))\n",
    "print(\"COMPLETENESS\")\n",
    "print(\"------\")\n",
    "print(\"avg:\",round(statistics.mean(kSC),2))\n",
    "print(\"std:\",round(statistics.stdev(kSC),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K medoids Euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclideanDistance(x, y):\n",
    "    squared_d = 0\n",
    "    for i in range(len(x)):\n",
    "        squared_d += (x[i] - y[i])**2\n",
    "    d = np.sqrt(squared_d)\n",
    "    return d\n",
    "\n",
    "class KMedoids:\n",
    "    def __init__(self, k = 2, max_iter = 100, has_converged = False):\n",
    "        ''' \n",
    "        Parameters\n",
    "        ----------\n",
    "        - k: number of clusters. \n",
    "        - max_iter: number of times centroids will move\n",
    "        - has_converged: to check if the algorithm stop or not\n",
    "        '''\n",
    "        self.k = k\n",
    "        self.max_iter = max_iter\n",
    "        self.has_converged = has_converged\n",
    "        self.medoids_cost = []\n",
    "        \n",
    "    def initMedoids(self, X):\n",
    "        ''' \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: input data. \n",
    "        '''\n",
    "        self.medoids = []\n",
    "        \n",
    "        #Starting medoids will be random members from data set X\n",
    "        indexes = np.random.randint(0, len(X)-1,self.k)\n",
    "        self.medoids = X[indexes]\n",
    "        \n",
    "        for i in range(0,self.k):\n",
    "            self.medoids_cost.append(0)\n",
    "        \n",
    "    def isConverged(self, new_medoids):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        new_medoids: the recently calculated medoids to be compared with the current medoids stored in the class\n",
    "        '''\n",
    "        return set([tuple(x) for x in self.medoids]) == set([tuple(x) for x in new_medoids])\n",
    "        \n",
    "    def updateMedoids(self, X, labels):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        labels: a list contains labels of data points\n",
    "        '''\n",
    "        self.has_converged = True\n",
    "        #Store data points to the current cluster they belong to\n",
    "        clusters = []\n",
    "        for i in range(0,self.k):\n",
    "            cluster = []\n",
    "            for j in range(len(X)):\n",
    "                if (labels[j] == i):\n",
    "                    cluster.append(X[j])\n",
    "            clusters.append(cluster)\n",
    "        #Calculate the new medoids\n",
    "        new_medoids = []\n",
    "        for i in range(0, self.k):\n",
    "            new_medoid = self.medoids[i]\n",
    "            old_medoids_cost = self.medoids_cost[i]\n",
    "            for j in range(len(clusters[i])):         # non-medoid being considered as medoid\n",
    "                #Cost of the current data points to be compared with the current optimal cost\n",
    "                cur_medoids_cost = 0\n",
    "                for dpoint_index in range(len(clusters[i])):\n",
    "                    cur_medoids_cost += euclideanDistance(clusters[i][j], clusters[i][dpoint_index])\n",
    "                #If current cost is less than current optimal cost,\n",
    "                #make the current data point new medoid of the cluster\n",
    "                if cur_medoids_cost < old_medoids_cost:\n",
    "                    new_medoid = clusters[i][j]\n",
    "                    old_medoids_cost = cur_medoids_cost\n",
    "            #Now we have the optimal medoid of the current cluster\n",
    "            new_medoids.append(new_medoid)\n",
    "        #If not converged yet, accept the new medoids\n",
    "        if not self.isConverged(new_medoids):\n",
    "            self.medoids = new_medoids\n",
    "            self.has_converged = False\n",
    "    \n",
    "    def fit(self, X):\n",
    "        '''\n",
    "        FIT function, used to find clusters\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: input data. \n",
    "        '''\n",
    "        self.initMedoids(X)\n",
    "        for i in range(self.max_iter):\n",
    "            #Labels for this iteration\n",
    "            cur_labels = []\n",
    "            for k in range(len(X)):\n",
    "                #Distances from a data point to each of the medoids\n",
    "                d_list = []                    \n",
    "                for j in range(0,self.k):\n",
    "                    d_list.append(euclideanDistance(self.medoids[j], X[k]))\n",
    "                #Data points' label is the medoid which has minimal distance to it\n",
    "                idx=d_list.index(min(d_list))\n",
    "                cur_labels.append(idx)\n",
    "                self.medoids_cost[idx]+=min(d_list)                    \n",
    "            self.updateMedoids(X, cur_labels)\n",
    "            if self.has_converged:\n",
    "                break\n",
    "        return np.array(self.medoids)\n",
    "\n",
    "        \n",
    "    def predict(self,data):\n",
    "        ''' \n",
    "        Parameters\n",
    "        ----------\n",
    "        data: input data.\n",
    "        \n",
    "        Returns:\n",
    "        ----------\n",
    "        pred: list cluster indexes of input data \n",
    "        '''\n",
    "        pred = []\n",
    "        for i in range(len(data)):\n",
    "            #Distances from a data point to each of the medoids\n",
    "            d_list = []\n",
    "            for j in range(len(self.medoids)):\n",
    "                d_list.append(euclideanDistance(self.medoids[j],data[i]))\n",
    "            pred.append(d_list.index(min(d_list))+1)\n",
    "        return np.array(pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-medoids Clustering with Euclidean Distance:\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e43961022da49d2a0834ef31e39d7d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "              Dataset       ARI       AMI       FMS  Homogenity  Completeness\n",
      "0          Colposcopy  0.092428  0.108421  0.280708    0.143109  1.471142e-01\n",
      "1              ECG200  0.117094  0.053091  0.670775    0.049630  6.824522e-02\n",
      "2          Lightning2  0.007257  0.008986  0.511475    0.015291  1.487590e-02\n",
      "3  SharePriceIncrease -0.000563 -0.000407  0.754090    0.000313  4.389560e-02\n",
      "4               Wafer  0.000009 -0.001076  0.687585    0.000001  6.664530e-07\n"
     ]
    }
   ],
   "source": [
    "print(\"K-medoids Clustering with Euclidean Distance:\")\n",
    "print(\"--------------------------------------------\")\n",
    "res={\"Dataset\":[],\"ARI\":[],\"AMI\":[],\"FMS\":[],\"Homogenity\":[],\"Completeness\":[]}\n",
    "kMEARI,kMEAMI,kMEFMS,kMEH,kMEC=[],[],[],[],[]\n",
    "for i in trange(len(datasets)):\n",
    "    data=datasets[i]\n",
    "    dataset=pd.read_csv(data[0])\n",
    "    res[\"Dataset\"].append(data[0][:-4])\n",
    "    cols=dataset.shape[1]\n",
    "    x=dataset.iloc[:,:cols-1].values\n",
    "    y=dataset.iloc[:,-1].values\n",
    "    \n",
    "    np.random.seed(10)\n",
    "    num_clusters=data[1]\n",
    "    model=KMedoids(k=num_clusters)\n",
    "    model.fit(x)\n",
    "    y_pred = model.predict(x)\n",
    "    \n",
    "    ARI=adjusted_rand_score(y,y_pred)\n",
    "    res[\"ARI\"].append(ARI)\n",
    "    AMI=adjusted_mutual_info_score(y,y_pred,average_method='arithmetic')\n",
    "    res[\"AMI\"].append(AMI)\n",
    "    FMI=fowlkes_mallows_score(y,y_pred,sparse=False)\n",
    "    res[\"FMS\"].append(FMI)\n",
    "    HOMOGENITY=homogeneity_score(y,y_pred)\n",
    "    res[\"Homogenity\"].append(HOMOGENITY)\n",
    "    COMPLETENESS=completeness_score(y,y_pred)\n",
    "    res[\"Completeness\"].append(COMPLETENESS)\n",
    "    \n",
    "    kMEARI.append(ARI)\n",
    "    kMEAMI.append(AMI)\n",
    "    kMEFMS.append(FMI)\n",
    "    kMEH.append(HOMOGENITY)\n",
    "    kMEC.append(COMPLETENESS)\n",
    "    \n",
    "df=pd.DataFrame(res)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average and Standard deviation of different metrics\n",
      "ARI\n",
      "------\n",
      "avg: 0.04\n",
      "std: 0.06\n",
      "AMI\n",
      "------\n",
      "avg: 0.03\n",
      "std: 0.05\n",
      "FMS\n",
      "------\n",
      "avg: 0.58\n",
      "std: 0.19\n",
      "HOMOGENITY\n",
      "------\n",
      "avg: 0.04\n",
      "std: 0.06\n",
      "COMPLETENESS\n",
      "------\n",
      "avg: 0.05\n",
      "std: 0.06\n"
     ]
    }
   ],
   "source": [
    "print(\"Average and Standard deviation of different metrics\")\n",
    "print(\"ARI\")\n",
    "print(\"------\")\n",
    "print(\"avg:\",round(statistics.mean(kMEARI),2))\n",
    "print(\"std:\",round(statistics.stdev(kMEARI),2))\n",
    "print(\"AMI\")\n",
    "print(\"------\")\n",
    "print(\"avg:\",round(statistics.mean(kMEAMI),2))\n",
    "print(\"std:\",round(statistics.stdev(kMEAMI),2))\n",
    "print(\"FMS\")\n",
    "print(\"------\")\n",
    "print(\"avg:\",round(statistics.mean(kMEFMS),2))\n",
    "print(\"std:\",round(statistics.stdev(kMEFMS),2))\n",
    "print(\"HOMOGENITY\")\n",
    "print(\"------\")\n",
    "print(\"avg:\",round(statistics.mean(kMEH),2))\n",
    "print(\"std:\",round(statistics.stdev(kMEH),2))\n",
    "print(\"COMPLETENESS\")\n",
    "print(\"------\")\n",
    "print(\"avg:\",round(statistics.mean(kMEC),2))\n",
    "print(\"std:\",round(statistics.stdev(kMEC),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuzzy C Means Euclidean Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Desktop\\fuzzycmeans1.png)\n",
    "Where, µ is fuzzy membership value of the data point, m is the fuzziness parameter (generally taken as 2), and xk is the data point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Desktop\\fuzzycmeans2.png)\n",
    "for updation of membership values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import operator\n",
    "import math\n",
    "\n",
    "class FuzzyCMeans:\n",
    "    \n",
    "    def __init__(self, df, num_clusters, max_iter=3):\n",
    "        self.df=df\n",
    "        self.num_attr=len(df.columns)-1  #no. of cols\n",
    "        self.k=num_clusters              #no. of clusters\n",
    "        self.MAX_ITER=max_iter           #Max. no. of iterations\n",
    "        self.n=len(df)                   #No. of data points\n",
    "        self.m=2.00                      #Fuzzy parameter\n",
    "        \n",
    "    #initializing membership matrix with random values\n",
    "    def initialize_membershipMatrix(self):     \n",
    "        membership_mat=list()\n",
    "        for i in range(self.n):\n",
    "            random_num_list=[random.random() for i in range(self.k)]\n",
    "            summ=sum(random_num_list)\n",
    "            temp=[x/summ for x in random_num_list]\n",
    "            membership_mat.append(temp)\n",
    "        return membership_mat\n",
    "    #Each data point lies in all the clusters available with some membership value \n",
    "    #it will be random in the initial state\n",
    "    \n",
    "    #cluster center is calculated in every iteration\n",
    "    def calculate_clusterCenter(self,membership_mat):     \n",
    "        cluster_mem_value=list(zip(*membership_mat)) #zip returns an iterator of tuples with each tuple having elements from all the iterables\n",
    "        cluster_centers=list()\n",
    "        for j in range(self.k):\n",
    "            x=list(cluster_mem_value[j])\n",
    "            xraised=[e**self.m for e in x]\n",
    "            denominator=sum(xraised)\n",
    "            temp=list()\n",
    "            for i in range(self.n):\n",
    "                datapoint=list(self.df.iloc[i])\n",
    "                prod=[xraised[i]*val for val in datapoint]\n",
    "                temp.append(prod)\n",
    "            numerator=map(sum,list(zip(*temp)))      \n",
    "            center=[z/denominator for z in numerator]\n",
    "            cluster_centers.append(center)\n",
    "        return cluster_centers\n",
    "    \n",
    "    #updating membership values using the recent cluster centers available\n",
    "    def update_membershipValue(self,membership_mat,cluster_centers):\n",
    "        p=float(2/(self.m-1))\n",
    "        for i in range(self.n):\n",
    "            x=list(self.df.iloc[i])\n",
    "            #finding out distance of each point from centroids (euclidean distance)\n",
    "            distances=[np.linalg.norm(list(map(operator.sub,x,cluster_centers[j]))) for j in range(self.k)]  \n",
    "            for j in range(self.k):\n",
    "                # finding new membership value for point i\n",
    "                den=sum([math.pow(float(distances[j]/distances[c]),p) for c in range(self.k)]) \n",
    "                membership_mat[i][j]=float(1/den)\n",
    "        return membership_mat\n",
    "    \n",
    "    #returns the cluster labels for each point in the dataset\n",
    "    def getClusters(self,membership_mat):\n",
    "        cluster_labels=list()\n",
    "        for i in range(self.n):\n",
    "            max_value,idx=max((val,idx) for (idx,val) in enumerate(membership_mat[i]))\n",
    "            cluster_labels.append(idx+1)\n",
    "        return cluster_labels\n",
    "    \n",
    "    def fit(self):\n",
    "        membership_mat=self.initialize_membershipMatrix()\n",
    "        for i in range(self.MAX_ITER):\n",
    "            cluster_centers=self.calculate_clusterCenter(membership_mat)\n",
    "            membership_mat=self.update_membershipValue(membership_mat,cluster_centers)\n",
    "            cluster_labels=self.getClusters(membership_mat)\n",
    "        return cluster_labels,cluster_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fuzzy C Means Clustering with Euclidean Distance:\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e39c0dcdcf4430d8b20217644712d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "              Dataset       ARI       AMI       FMS  Homogenity  Completeness\n",
      "0          Colposcopy  0.111517  0.132553  0.325275    0.159920      0.183650\n",
      "1              ECG200 -0.005263 -0.003760  0.527580    0.000081      0.000076\n",
      "2          Lightning2  0.000096  0.002618  0.507950    0.008874      0.008633\n",
      "3  SharePriceIncrease -0.000425 -0.000380  0.533276    0.000016      0.000014\n",
      "4               Wafer  0.085454  0.050653  0.701145    0.078170      0.038559\n"
     ]
    }
   ],
   "source": [
    "print(\"Fuzzy C Means Clustering with Euclidean Distance:\")\n",
    "print(\"--------------------------------------------\")\n",
    "res={\"Dataset\":[],\"ARI\":[],\"AMI\":[],\"FMS\":[],\"Homogenity\":[],\"Completeness\":[]}\n",
    "cMARI,cMAMI,cMFMS,cMH,cMC=[],[],[],[],[]\n",
    "for i in trange(len(datasets)):\n",
    "    data=datasets[i]\n",
    "    dataset=pd.read_csv(data[0])\n",
    "    res[\"Dataset\"].append(data[0][:-4])\n",
    "    cols=dataset.shape[1]\n",
    "    x=dataset.iloc[:,:cols-1].values\n",
    "    y=dataset.iloc[:,-1].values\n",
    "    \n",
    "    np.random.seed(10)\n",
    "    num_clusters=data[1]\n",
    "    fcm=FuzzyCMeans(dataset,num_clusters)\n",
    "    y_pred,centers=fcm.fit()\n",
    "    \n",
    "    ARI=adjusted_rand_score(y,y_pred)\n",
    "    res[\"ARI\"].append(ARI)\n",
    "    AMI=adjusted_mutual_info_score(y,y_pred,average_method='arithmetic')\n",
    "    res[\"AMI\"].append(AMI)\n",
    "    FMI=fowlkes_mallows_score(y,y_pred,sparse=False)\n",
    "    res[\"FMS\"].append(FMI)\n",
    "    HOMOGENITY=homogeneity_score(y,y_pred)\n",
    "    res[\"Homogenity\"].append(HOMOGENITY)\n",
    "    COMPLETENESS=completeness_score(y,y_pred)\n",
    "    res[\"Completeness\"].append(COMPLETENESS)\n",
    "    \n",
    "    cMARI.append(ARI)\n",
    "    cMAMI.append(AMI)\n",
    "    cMFMS.append(FMI)\n",
    "    cMH.append(HOMOGENITY)\n",
    "    cMC.append(COMPLETENESS)\n",
    "    \n",
    "df=pd.DataFrame(res)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average and Standard deviation of different metrics\n",
      "ARI\n",
      "------\n",
      "avg: 0.04\n",
      "std: 0.06\n",
      "AMI\n",
      "------\n",
      "avg: 0.04\n",
      "std: 0.06\n",
      "FMS\n",
      "------\n",
      "avg: 0.52\n",
      "std: 0.13\n",
      "HOMOGENITY\n",
      "------\n",
      "avg: 0.05\n",
      "std: 0.07\n",
      "COMPLETENESS\n",
      "------\n",
      "avg: 0.05\n",
      "std: 0.08\n"
     ]
    }
   ],
   "source": [
    "print(\"Average and Standard deviation of different metrics\")\n",
    "print(\"ARI\")\n",
    "print(\"------\")\n",
    "print(\"avg:\",round(statistics.mean(cMARI),2))\n",
    "print(\"std:\",round(statistics.stdev(cMARI),2))\n",
    "print(\"AMI\")\n",
    "print(\"------\")\n",
    "print(\"avg:\",round(statistics.mean(cMAMI),2))\n",
    "print(\"std:\",round(statistics.stdev(cMAMI),2))\n",
    "print(\"FMS\")\n",
    "print(\"------\")\n",
    "print(\"avg:\",round(statistics.mean(cMFMS),2))\n",
    "print(\"std:\",round(statistics.stdev(cMFMS),2))\n",
    "print(\"HOMOGENITY\")\n",
    "print(\"------\")\n",
    "print(\"avg:\",round(statistics.mean(cMH),2))\n",
    "print(\"std:\",round(statistics.stdev(cMH),2))\n",
    "print(\"COMPLETENESS\")\n",
    "print(\"------\")\n",
    "print(\"avg:\",round(statistics.mean(cMC),2))\n",
    "print(\"std:\",round(statistics.stdev(cMC),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density Peaks with Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DensityPeakCluster(object):\n",
    "    \"\"\"\n",
    "    Attributes:\n",
    "        n_id: data row count\n",
    "        distance: each id distance\n",
    "        dc: threshold of density cut off\n",
    "        rho: each id density\n",
    "        nneigh: each id min upper density nearest neighbor\n",
    "        delta: each id min upper density nearest neighbor distance\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x, k, distance_metric='euclidean'):\n",
    "        \"\"\"\n",
    "        Init parameters for Density peak cluster.\n",
    "        parameters\n",
    "        x: data\n",
    "        k: number of clusters\n",
    "        distance_metric: distance calculate function euclidean\n",
    "        \"\"\"\n",
    "        self.data = x\n",
    "        self.k = k\n",
    "        self.distance_metric = distance_metric\n",
    "        \n",
    "\n",
    "    def build_distance(self):\n",
    "        \"\"\"\n",
    "        Calculates distance dictionary.\n",
    "        return: distance dict, max distance, min distance\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import pdist, squareform\n",
    "        \n",
    "        # Pairwise distances between observations in n-dimensional space.\n",
    "        distance_matrix = pdist(self.data, metric=self.distance_metric) \n",
    "        \n",
    "        # Convert a vector-form distance vector to a square-form distance matrix\n",
    "        distance_matrix = squareform(distance_matrix)\n",
    "        \n",
    "        # Return the indices for the upper-triangle of an (n, m) array.\n",
    "        triangle_upper = np.triu_indices(self.data.shape[0], 1)\n",
    "        triangle_upper = distance_matrix[triangle_upper]\n",
    "\n",
    "        # distance dictionary\n",
    "        distance = {}\n",
    "        for i in range(self.n_id):\n",
    "            for j in range(i + 1, self.n_id):\n",
    "                distance[(i, j)] = distance_matrix[i, j]\n",
    "                distance[(j, i)] = distance_matrix[i, j]\n",
    "\n",
    "        max_dis, min_dis = np.max(triangle_upper), np.min(triangle_upper)\n",
    "        return distance, max_dis, min_dis\n",
    "\n",
    "    def select_dc(self):\n",
    "        \"\"\"\n",
    "        selects the local density threshold that let average neighbor is 1-2 percent of all nodes.\n",
    "        return: dc that local density threshold\n",
    "        \"\"\"\n",
    "        max_dis, min_dis = self.max_dis, self.min_dis\n",
    "        dc = (max_dis + min_dis) / 2\n",
    "        \n",
    "        while True:\n",
    "            # calculating the nearest neighbors within the selected dc\n",
    "            nneighs = sum([1 for v in self.distances.values() if v < dc]) / self.n_id ** 2\n",
    "            if 0.01 <= nneighs <= 0.002:\n",
    "                break\n",
    "            # binary search\n",
    "            if nneighs < 0.01:\n",
    "                min_dis = dc\n",
    "            else:\n",
    "                max_dis = dc\n",
    "            dc = (max_dis + min_dis) / 2\n",
    "            if max_dis - min_dis < 0.0001:\n",
    "                break\n",
    "        return dc\n",
    "\n",
    "\n",
    "    def local_density(self):\n",
    "        \"\"\"\n",
    "        computes all points' local density.\n",
    "        return: local density vector that index is the point index\n",
    "        \"\"\"\n",
    "        cutoff_func = lambda dij, dc: 1 if dij < dc else 0\n",
    "        \n",
    "        rho = [0] * self.n_id\n",
    "        for i in range(self.n_id):\n",
    "            for j in range(i + 1, self.n_id):\n",
    "                temp = cutoff_func(self.distances[(i, j)], self.dc)\n",
    "                rho[i] += temp\n",
    "                rho[j] += temp\n",
    "        return np.array(rho, np.float32)\n",
    "\n",
    "    \n",
    "    def clustering(self):\n",
    "        \"\"\"\n",
    "        Compute all points' min util to the higher local density point(which is the nearest neighbor).\n",
    "        return: y_pred containing labels for all the points\n",
    "        \"\"\"\n",
    "        # sort_rho_idx contains index of values in rho in descending order\n",
    "        sort_rho_idx = np.argsort(-self.rho)\n",
    "        \n",
    "        # nneigh contains each id min upper density nearest neighbor\n",
    "        # delta contains each id min upper density nearest neighbor distance\n",
    "        delta, nneigh = [float(self.max_dis)] * (self.n_id), [0] * self.n_id\n",
    "        delta[sort_rho_idx[0]] = -1\n",
    "        for i in range(self.n_id):\n",
    "            for j in range(0, i):\n",
    "                old_i, old_j = sort_rho_idx[i], sort_rho_idx[j]\n",
    "                if self.distances[(old_i, old_j)] < delta[old_i]:\n",
    "                    delta[old_i] = self.distances[(old_i, old_j)]\n",
    "                    nneigh[old_i] = old_j\n",
    "        delta[sort_rho_idx[0]] = max(delta)\n",
    "    \n",
    "        # cluster center selection\n",
    "        # vector multiplication of values in rho and delta\n",
    "        res=np.multiply(self.rho,delta)\n",
    "        \n",
    "        # points contains index of values in res in descending order\n",
    "        points=np.argsort(-res)\n",
    "        \n",
    "        # idxs contains indexes of top k points as cluster centers\n",
    "        idxs=points[:self.k]\n",
    "        centers=[]\n",
    "        for i in range(self.k):\n",
    "            centers.append(self.data[idxs[i]])\n",
    "            \n",
    "        # assignment of clusters to all the points \n",
    "        clusters={}\n",
    "        #assignment of labels to the cluster centers\n",
    "        for i in range(self.k):\n",
    "            clusters[idxs[i]]=i\n",
    "        #assignment of labels to all the points other than cluster centers based on the nearest neighbors\n",
    "        for j in range(self.n_id):\n",
    "            if sort_rho_idx[j] not in clusters:\n",
    "                clusters[sort_rho_idx[j]]=clusters[nneigh[sort_rho_idx[j]]]\n",
    "\n",
    "        # predicting the labels for all the points and placing them in y_pred\n",
    "        y_pred=[]\n",
    "        for i in range(self.n_id):\n",
    "            y_pred.append(clusters[i]+1)\n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "    def fit(self,data):\n",
    "        \"\"\"\n",
    "        Fits the model.\n",
    "        param data: data for clustering\n",
    "        return: predicted labels\n",
    "        \"\"\"\n",
    "        if isinstance(data, np.ndarray): data = np.array(data)\n",
    "\n",
    "        self.n_id = self.data.shape[0]\n",
    "\n",
    "        # calculate distance\n",
    "        self.distances, self.max_dis, self.min_dis = self.build_distance()\n",
    "\n",
    "        # select distance cut off\n",
    "        self.dc = self.select_dc()\n",
    "\n",
    "        # calculate local density\n",
    "        self.rho = self.local_density()\n",
    "        \n",
    "        # calculate nearest neighbor and delta and performs actual clustering\n",
    "        y_pred = self.clustering()\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Density Peaks Clustering with Euclidean Distance:\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4502753ffe345d1b9be1747bb5c9797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "              Dataset       ARI       AMI       FMS  Homogenity  Completeness\n",
      "0          Colposcopy  0.018208  0.051353  0.400210    0.069189      0.209473\n",
      "1              ECG200  0.198014  0.108872  0.664993    0.106894      0.118946\n",
      "2          Lightning2  0.002352 -0.014167  0.708191    0.000546      0.004355\n",
      "3  SharePriceIncrease -0.010791  0.000494  0.689611    0.000780      0.001662\n",
      "4               Wafer -0.002467 -0.001043  0.687234    0.000051      0.000027\n"
     ]
    }
   ],
   "source": [
    "print(\"Density Peaks Clustering with Euclidean Distance:\")\n",
    "print(\"--------------------------------------------\")\n",
    "res={\"Dataset\":[],\"ARI\":[],\"AMI\":[],\"FMS\":[],\"Homogenity\":[],\"Completeness\":[]}\n",
    "dEARI,dEAMI,dEFMS,dEH,dEC=[],[],[],[],[]\n",
    "for i in trange(len(datasets)):\n",
    "    data=datasets[i]\n",
    "    dataset=pd.read_csv(data[0])\n",
    "    res[\"Dataset\"].append(data[0][:-4])\n",
    "    cols=dataset.shape[1]\n",
    "    x=dataset.iloc[:,:cols-1].values\n",
    "    y=dataset.iloc[:,-1].values\n",
    "    \n",
    "    num_clusters=data[1]\n",
    "    dpca = DensityPeakCluster(x,num_clusters)\n",
    "    y_pred=dpca.fit(x)\n",
    "    \n",
    "    ARI=adjusted_rand_score(y,y_pred)\n",
    "    res[\"ARI\"].append(ARI)\n",
    "    AMI=adjusted_mutual_info_score(y,y_pred,average_method='arithmetic')\n",
    "    res[\"AMI\"].append(AMI)\n",
    "    FMI=fowlkes_mallows_score(y,y_pred,sparse=False)\n",
    "    res[\"FMS\"].append(FMI)\n",
    "    HOMOGENITY=homogeneity_score(y,y_pred)\n",
    "    res[\"Homogenity\"].append(HOMOGENITY)\n",
    "    COMPLETENESS=completeness_score(y,y_pred)\n",
    "    res[\"Completeness\"].append(COMPLETENESS)\n",
    "    \n",
    "    dEARI.append(ARI)\n",
    "    dEAMI.append(AMI)\n",
    "    dEFMS.append(FMI)\n",
    "    dEH.append(HOMOGENITY)\n",
    "    dEC.append(COMPLETENESS)\n",
    "    \n",
    "df=pd.DataFrame(res)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average and Standard deviation of different metrics\n",
      "ARI\n",
      "------\n",
      "avg: 0.04\n",
      "std: 0.09\n",
      "AMI\n",
      "------\n",
      "avg: 0.03\n",
      "std: 0.05\n",
      "FMS\n",
      "------\n",
      "avg: 0.63\n",
      "std: 0.13\n",
      "HOMOGENITY\n",
      "------\n",
      "avg: 0.04\n",
      "std: 0.05\n",
      "COMPLETENESS\n",
      "------\n",
      "avg: 0.07\n",
      "std: 0.09\n"
     ]
    }
   ],
   "source": [
    "print(\"Average and Standard deviation of different metrics\")\n",
    "print(\"ARI\")\n",
    "print(\"------\")\n",
    "print(\"avg:\",round(statistics.mean(dEARI),2))\n",
    "print(\"std:\",round(statistics.stdev(dEARI),2))\n",
    "print(\"AMI\")\n",
    "print(\"------\")\n",
    "print(\"avg:\",round(statistics.mean(dEAMI),2))\n",
    "print(\"std:\",round(statistics.stdev(dEAMI),2))\n",
    "print(\"FMS\")\n",
    "print(\"------\")\n",
    "print(\"avg:\",round(statistics.mean(dEFMS),2))\n",
    "print(\"std:\",round(statistics.stdev(dEFMS),2))\n",
    "print(\"HOMOGENITY\")\n",
    "print(\"------\")\n",
    "print(\"avg:\",round(statistics.mean(dEH),2))\n",
    "print(\"std:\",round(statistics.stdev(dEH),2))\n",
    "print(\"COMPLETENESS\")\n",
    "print(\"------\")\n",
    "print(\"avg:\",round(statistics.mean(dEC),2))\n",
    "print(\"std:\",round(statistics.stdev(dEC),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density Peaks with DTW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def DTW_Distance(s1,s2):\n",
    "    DTW={}\n",
    "    \n",
    "    w = max(int(0.05*len(s1)), abs(len(s1)-len(s2)))\n",
    "   \n",
    "    for i in range(-1,len(s1)):\n",
    "        for j in range(-1,len(s2)):\n",
    "            DTW[(i, j)] = float('inf')\n",
    "    DTW[(-1, -1)] = 0\n",
    "  \n",
    "    for i in range(len(s1)):\n",
    "        for j in range(max(0, i-w), min(len(s2), i+w)):\n",
    "            dist= (s1[i]-s1[j])**2\n",
    "            DTW[(i, j)] = dist + min(DTW[(i-1, j)],DTW[(i, j-1)], DTW[(i-1, j-1)])\n",
    "            \n",
    "    return np.sqrt(DTW[len(s1)-1, len(s2)-1])\n",
    "\n",
    "class DensityPeakClusterD(object):\n",
    "    \"\"\"\n",
    "    Attributes:\n",
    "        n_id: data row count\n",
    "        distance: each id distance\n",
    "        dc: threshold of density cut off\n",
    "        rho: each id density\n",
    "        nneigh: each id min upper density nearest neighbor\n",
    "        delta: each id min upper density nearest neighbor distance\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x, k, distance_metric='dtw'):\n",
    "        \"\"\"\n",
    "        Init parameters for Density peak cluster.\n",
    "        parameters\n",
    "        x: data\n",
    "        k: number of clusters\n",
    "        distance_metric: distance calculate function euclidean\n",
    "        \"\"\"\n",
    "        self.data = x\n",
    "        self.k = k\n",
    "        self.distance_metric = distance_metric\n",
    "        \n",
    "\n",
    "    def build_distance(self):\n",
    "        \"\"\"\n",
    "        Calculates distance dicttionary.\n",
    "        return: distance dict, max distance, min distance\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import pdist, squareform\n",
    "        \n",
    "        # Pairwise distances between observations in n-dimensional space.\n",
    "        distance_matrix = pdist(self.data, DTW_Distance)\n",
    "        \n",
    "        # Convert a vector-form distance vector to a square-form distance matrix\n",
    "        distance_matrix = squareform(distance_matrix)\n",
    "        \n",
    "        # Return the indices for the upper-triangle of an (n, m) array.\n",
    "        triangle_upper = np.triu_indices(self.data.shape[0], 1)\n",
    "        triangle_upper = distance_matrix[triangle_upper]\n",
    "\n",
    "        # distance dictionary\n",
    "        distance = {}\n",
    "        for i in range(self.n_id):\n",
    "            for j in range(i + 1, self.n_id):\n",
    "                distance[(i, j)] = distance_matrix[i, j]\n",
    "                distance[(j, i)] = distance_matrix[i, j]\n",
    "\n",
    "        max_dis, min_dis = np.max(triangle_upper), np.min(triangle_upper)\n",
    "        return distance, max_dis, min_dis\n",
    "\n",
    "    def select_dc(self):\n",
    "        \"\"\"\n",
    "        selects the local density threshold that let average neighbor is 1-2 percent of all nodes.\n",
    "        return: dc that local density threshold\n",
    "        \"\"\"\n",
    "        max_dis, min_dis = self.max_dis, self.min_dis\n",
    "        dc = (max_dis + min_dis) / 2\n",
    "        \n",
    "        while True:\n",
    "            # calculating the nearest neighbors within the selected dc\n",
    "            nneighs = sum([1 for v in self.distances.values() if v < dc]) / self.n_id ** 2\n",
    "            if 0.01 <= nneighs <= 0.002:\n",
    "                break\n",
    "            # binary search\n",
    "            if nneighs < 0.01:\n",
    "                min_dis = dc\n",
    "            else:\n",
    "                max_dis = dc\n",
    "            dc = (max_dis + min_dis) / 2\n",
    "            if max_dis - min_dis < 0.0001:\n",
    "                break\n",
    "        return dc\n",
    "\n",
    "\n",
    "    def local_density(self):\n",
    "        \"\"\"\n",
    "        computes all points' local density.\n",
    "        return: local density vector that index is the point index\n",
    "        \"\"\"\n",
    "        cutoff_func = lambda dij, dc: 1 if dij < dc else 0\n",
    "        \n",
    "        rho = [0] * self.n_id\n",
    "        for i in range(self.n_id):\n",
    "            for j in range(i + 1, self.n_id):\n",
    "                temp = cutoff_func(self.distances[(i, j)], self.dc)\n",
    "                rho[i] += temp\n",
    "                rho[j] += temp\n",
    "        return np.array(rho, np.float32)\n",
    "\n",
    "    \n",
    "    def clustering(self):\n",
    "        \"\"\"\n",
    "        Compute all points' min util to the higher local density point(which is the nearest neighbor).\n",
    "        return: y_pred containing labels for all the points\n",
    "        \"\"\"\n",
    "        # sort_rho_idx contains index of values in rho in descending order\n",
    "        sort_rho_idx = np.argsort(-self.rho)\n",
    "        \n",
    "        # nneigh contains each id min upper density nearest neighbor\n",
    "        # delta contains each id min upper density nearest neighbor distance\n",
    "        delta, nneigh = [float(self.max_dis)] * (self.n_id), [0] * self.n_id\n",
    "        delta[sort_rho_idx[0]] = -1\n",
    "        for i in range(self.n_id):\n",
    "            for j in range(0, i):\n",
    "                old_i, old_j = sort_rho_idx[i], sort_rho_idx[j]\n",
    "                if self.distances[(old_i, old_j)] < delta[old_i]:\n",
    "                    delta[old_i] = self.distances[(old_i, old_j)]\n",
    "                    nneigh[old_i] = old_j\n",
    "        delta[sort_rho_idx[0]] = max(delta)\n",
    "    \n",
    "        # cluster center selection\n",
    "        # vector multiplication of values in rho and delta\n",
    "        res=np.multiply(self.rho,delta)\n",
    "        \n",
    "        # points contains index of values in res in descending order\n",
    "        points=np.argsort(-res)\n",
    "        \n",
    "        # idxs contains indexes of top k points as cluster centers\n",
    "        idxs=points[:self.k]\n",
    "        '''centers=[]\n",
    "        for i in range(self.k):\n",
    "            centers.append(self.data[idxs[i]])'''\n",
    "            \n",
    "        # assignment of clusters to all the points \n",
    "        clusters={}\n",
    "        #assignment of labels to the cluster centers\n",
    "        for i in range(self.k):\n",
    "            clusters[idxs[i]]=i\n",
    "        #assignment of labels to all the points other than cluster centers based on the nearest neighbors\n",
    "        for j in range(self.n_id):\n",
    "            if sort_rho_idx[j] not in clusters:\n",
    "                clusters[sort_rho_idx[j]]=clusters[nneigh[sort_rho_idx[j]]]\n",
    "\n",
    "        # predicting the labels for all the points and placing them in y_pred\n",
    "        y_pred=[]\n",
    "        for i in range(self.n_id):\n",
    "            y_pred.append(clusters[i]+1)\n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "    def fit(self,data):\n",
    "        \"\"\"\n",
    "        Fits the model.\n",
    "        param data: data for clustering\n",
    "        return: predicted labels\n",
    "        \"\"\"\n",
    "        if isinstance(data, np.ndarray): data = np.array(data)\n",
    "\n",
    "        self.n_id = self.data.shape[0]\n",
    "\n",
    "        # calculate distance\n",
    "        self.distances, self.max_dis, self.min_dis = self.build_distance()\n",
    "\n",
    "        # select distance cut off\n",
    "        self.dc = self.select_dc()\n",
    "\n",
    "        # calculate local density\n",
    "        self.rho = self.local_density()\n",
    "        \n",
    "        # calculate nearest neighbor and delta and performs actual clustering\n",
    "        y_pred = self.clustering()\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Density Peaks Clustering with DTW Distance:\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9075325eecd0407fb35c3c8c04fa18c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "              Dataset       ARI       AMI       FMS  Homogenity  Completeness\n",
      "0          Colposcopy  0.002376  0.002889  0.434005    0.026694      0.287088\n",
      "1              ECG200 -0.004951 -0.003485  0.737881    0.003209      0.065001\n",
      "2          Lightning2  0.008599  0.005953  0.716218    0.011455      0.160738\n",
      "3  SharePriceIncrease  0.013844  0.009775  0.762866    0.010113      0.194859\n",
      "4               Wafer -0.008716 -0.006431  0.917709    0.001499      0.013279\n"
     ]
    }
   ],
   "source": [
    "print(\"Density Peaks Clustering with DTW Distance:\")\n",
    "print(\"--------------------------------------------\")\n",
    "res={\"Dataset\":[],\"ARI\":[],\"AMI\":[],\"FMS\":[],\"Homogenity\":[],\"Completeness\":[]}\n",
    "dDARI,dDAMI,dDFMS,dDH,dDC=[],[],[],[],[]\n",
    "for i in trange(len(datasets)):\n",
    "    data=datasets[i]\n",
    "    dataset=pd.read_csv(data[0])\n",
    "    res[\"Dataset\"].append(data[0][:-4])\n",
    "    \n",
    "    cols=dataset.shape[1]\n",
    "    x=dataset.iloc[:,:cols-1].values\n",
    "    y=dataset.iloc[:,-1].values\n",
    "    \n",
    "    num_clusters=data[1]\n",
    "    dpcaD = DensityPeakClusterD(x,num_clusters)\n",
    "    y_pred=dpcaD.fit(x)\n",
    "    \n",
    "    ARI=adjusted_rand_score(y,y_pred)\n",
    "    res[\"ARI\"].append(ARI)\n",
    "    AMI=adjusted_mutual_info_score(y,y_pred,average_method='arithmetic')\n",
    "    res[\"AMI\"].append(AMI)\n",
    "    FMI=fowlkes_mallows_score(y,y_pred,sparse=False)\n",
    "    res[\"FMS\"].append(FMI)\n",
    "    HOMOGENITY=homogeneity_score(y,y_pred)\n",
    "    res[\"Homogenity\"].append(HOMOGENITY)\n",
    "    COMPLETENESS=completeness_score(y,y_pred)\n",
    "    res[\"Completeness\"].append(COMPLETENESS)\n",
    "    \n",
    "    dDARI.append(ARI)\n",
    "    dDAMI.append(AMI)\n",
    "    dDFMS.append(FMI)\n",
    "    dDH.append(HOMOGENITY)\n",
    "    dDC.append(COMPLETENESS)\n",
    "    \n",
    "df=pd.DataFrame(res)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average and Standard deviation of different metrics\n",
      "ARI\n",
      "------\n",
      "avg: 0.0\n",
      "std: 0.01\n",
      "AMI\n",
      "------\n",
      "avg: 0.0\n",
      "std: 0.01\n",
      "FMS\n",
      "------\n",
      "avg: 0.71\n",
      "std: 0.18\n",
      "HOMOGENITY\n",
      "------\n",
      "avg: 0.01\n",
      "std: 0.01\n",
      "COMPLETENESS\n",
      "------\n",
      "avg: 0.14\n",
      "std: 0.11\n"
     ]
    }
   ],
   "source": [
    "print(\"Average and Standard deviation of different metrics\")\n",
    "print(\"ARI\")\n",
    "print(\"------\")\n",
    "print(\"avg:\",round(statistics.mean(dDARI),2))\n",
    "print(\"std:\",round(statistics.stdev(dDARI),2))\n",
    "print(\"AMI\")\n",
    "print(\"------\")\n",
    "print(\"avg:\",round(statistics.mean(dDAMI),2))\n",
    "print(\"std:\",round(statistics.stdev(dDAMI),2))\n",
    "print(\"FMS\")\n",
    "print(\"------\")\n",
    "print(\"avg:\",round(statistics.mean(dDFMS),2))\n",
    "print(\"std:\",round(statistics.stdev(dDFMS),2))\n",
    "print(\"HOMOGENITY\")\n",
    "print(\"------\")\n",
    "print(\"avg:\",round(statistics.mean(dDH),2))\n",
    "print(\"std:\",round(statistics.stdev(dDH),2))\n",
    "print(\"COMPLETENESS\")\n",
    "print(\"------\")\n",
    "print(\"avg:\",round(statistics.mean(dDC),2))\n",
    "print(\"std:\",round(statistics.stdev(dDC),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agglomerative Clustering with Euclidean Distance:\n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddc1b87660bf4ce696624129c1b00a2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "              Dataset       ARI       AMI       FMS  Homogenity  Completeness\n",
      "0          Colposcopy  0.079482  0.092676  0.295376    0.125439      0.138992\n",
      "1              ECG200  0.244086  0.145752  0.671168    0.146076      0.152485\n",
      "2          Lightning2  0.145156  0.107958  0.661253    0.097720      0.138426\n",
      "3  SharePriceIncrease  0.029147  0.010744  0.734392    0.007221      0.027060\n",
      "4               Wafer  0.002483 -0.001016  0.687940    0.000088      0.000045\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "print(\"Agglomerative Clustering with Euclidean Distance:\")\n",
    "print(\"-------------------------------------------------\")\n",
    "res={\"Dataset\":[],\"ARI\":[],\"AMI\":[],\"FMS\":[],\"Homogenity\":[],\"Completeness\":[]}\n",
    "aARI,aAMI,aFMS,aH,aC=[],[],[],[],[]\n",
    "for i in trange(len(datasets)):\n",
    "    data=datasets[i]\n",
    "    dataset=pd.read_csv(data[0])\n",
    "    res[\"Dataset\"].append(data[0][:-4])\n",
    "    cols=dataset.shape[1]\n",
    "    x=dataset.iloc[:,:cols-1].values\n",
    "    y=dataset.iloc[:,-1].values\n",
    "    \n",
    "    num_clusters=data[1]\n",
    "    clustering = AgglomerativeClustering(n_clusters=num_clusters,affinity='euclidean',linkage='ward')\n",
    "    y_pred=clustering.fit_predict(x)\n",
    "    \n",
    "    ARI=adjusted_rand_score(y,y_pred)\n",
    "    res[\"ARI\"].append(ARI)\n",
    "    AMI=adjusted_mutual_info_score(y,y_pred,average_method='arithmetic')\n",
    "    res[\"AMI\"].append(AMI)\n",
    "    FMI=fowlkes_mallows_score(y,y_pred,sparse=False)\n",
    "    res[\"FMS\"].append(FMI)\n",
    "    HOMOGENITY=homogeneity_score(y,y_pred)\n",
    "    res[\"Homogenity\"].append(HOMOGENITY)\n",
    "    COMPLETENESS=completeness_score(y,y_pred)\n",
    "    res[\"Completeness\"].append(COMPLETENESS)\n",
    "    \n",
    "    aARI.append(ARI)\n",
    "    aAMI.append(AMI)\n",
    "    aFMS.append(FMI)\n",
    "    aH.append(HOMOGENITY)\n",
    "    aC.append(COMPLETENESS)\n",
    "    \n",
    "df=pd.DataFrame(res)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average and Standard deviation of different metrics\n",
      "ARI\n",
      "------\n",
      "avg: 0.1\n",
      "std: 0.1\n",
      "AMI\n",
      "------\n",
      "avg: 0.07\n",
      "std: 0.06\n",
      "FMS\n",
      "------\n",
      "avg: 0.61\n",
      "std: 0.18\n",
      "HOMOGENITY\n",
      "------\n",
      "avg: 0.08\n",
      "std: 0.07\n",
      "COMPLETENESS\n",
      "------\n",
      "avg: 0.09\n",
      "std: 0.07\n"
     ]
    }
   ],
   "source": [
    "print(\"Average and Standard deviation of different metrics\")\n",
    "print(\"ARI\")\n",
    "print(\"------\")\n",
    "print(\"avg:\",round(statistics.mean(aARI),2))\n",
    "print(\"std:\",round(statistics.stdev(aARI),2))\n",
    "print(\"AMI\")\n",
    "print(\"------\")\n",
    "print(\"avg:\",round(statistics.mean(aAMI),2))\n",
    "print(\"std:\",round(statistics.stdev(aAMI),2))\n",
    "print(\"FMS\")\n",
    "print(\"------\")\n",
    "print(\"avg:\",round(statistics.mean(aFMS),2))\n",
    "print(\"std:\",round(statistics.stdev(aFMS),2))\n",
    "print(\"HOMOGENITY\")\n",
    "print(\"------\")\n",
    "print(\"avg:\",round(statistics.mean(aH),2))\n",
    "print(\"std:\",round(statistics.stdev(aH),2))\n",
    "print(\"COMPLETENESS\")\n",
    "print(\"------\")\n",
    "print(\"avg:\",round(statistics.mean(aC),2))\n",
    "print(\"std:\",round(statistics.stdev(aC),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spread values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kmeans-euc - kmeans-dtw - 0.0005\n",
      "kmeans-euc - kmeans-shape - 0.0028\n",
      "kmeans-euc - kmedoids-euc - 0.0\n",
      "kmeans-euc - cmeans-euc - 0.0047\n",
      "kmeans-euc - dp-euc - 0.0023\n",
      "kmeans-euc - dp-dtw - 0.0047\n",
      "kmeans-euc - agglom-euc - 0.0069\n",
      "kmeans-dtw - kmeans-shape - 0.002\n",
      "kmeans-dtw - kmedoids-euc - 0.0007\n",
      "kmeans-dtw - cmeans-euc - 0.0062\n",
      "kmeans-dtw - dp-euc - 0.0027\n",
      "kmeans-dtw - dp-dtw - 0.0056\n",
      "kmeans-dtw - agglom-euc - 0.0041\n",
      "kmeans-shape - kmedoids-euc - 0.003\n",
      "kmeans-shape - cmeans-euc - 0.012\n",
      "kmeans-shape - dp-euc - 0.0007\n",
      "kmeans-shape - dp-dtw - 0.0095\n",
      "kmeans-shape - agglom-euc - 0.0027\n",
      "kmedoids-euc - cmeans-euc - 0.0045\n",
      "kmedoids-euc - dp-euc - 0.0024\n",
      "kmedoids-euc - dp-dtw - 0.0047\n",
      "kmedoids-euc - agglom-euc - 0.0072\n",
      "cmeans-euc - dp-euc - 0.0116\n",
      "cmeans-euc - dp-dtw - 0.0042\n",
      "cmeans-euc - agglom-euc - 0.0184\n",
      "dp-euc - dp-dtw - 0.0084\n",
      "dp-euc - agglom-euc - 0.0056\n",
      "dp-dtw - agglom-euc - 0.0174\n"
     ]
    }
   ],
   "source": [
    "methods = [kEARI, kDARI, kSARI, kMEARI, cMARI, dEARI, dDARI, aARI]\n",
    "method_names = ['kmeans-euc', 'kmeans-dtw', 'kmeans-shape', 'kmedoids-euc', 'cmeans-euc', 'dp-euc', 'dp-dtw', 'agglom-euc']\n",
    "for i in range(8):\n",
    "    method1 = methods[i]\n",
    "    for j in range(i+1,8):\n",
    "        method2 = methods[j]\n",
    "        spread=0\n",
    "        for k in range(5):\n",
    "            spread += ((method1[k]-method2[k])**2)\n",
    "        print(method_names[i],'-',method_names[j],'-',round(spread/5,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
